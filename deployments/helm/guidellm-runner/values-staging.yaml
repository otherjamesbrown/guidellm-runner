# Staging environment values for guidellm-runner
# Updated: 2026-01-08 - All enabled staging models

image:
  repository: ghcr.io/otherjamesbrown/ai-aas/guidellm-runner
  tag: "dev"
  pullPolicy: Always
  pullSecrets:
    - ghcr-pull-secret

# Use dedicated API key secret for benchmarking
apiKey:
  existingSecret: "guidellm-runner-api-key"
  existingSecretKey: "api-key"

config:
  logLevel: info

  defaults:
    profile: constant
    rate: 0.5  # Conservative rate to avoid quota limits
    interval: 300  # Run benchmark every 5 minutes
    maxSeconds: 30
    maxTokens: 50
    # Use emulated data with gpt2 tokenizer
    dataSpec: "type=emulated,tokenizer=gpt2,prompt_tokens=100,output_tokens=50"

  environments:
    staging:
      targets:
        # vLLM Models (OpenAI-compatible)
        - name: unsloth-gpt-oss-20b
          url: http://api-router-service-staging-api-router-service.staging.svc.cluster.local:8080
          model: unsloth/gpt-oss-20b
          rate: 0.5
          maxSeconds: 30
          tags:
            runtime: vllm
            gpu: rtx6000-blackwell

        - name: qwen2-vl-7b-instruct
          url: http://api-router-service-staging-api-router-service.staging.svc.cluster.local:8080
          model: qwen2-vl-7b-instruct
          rate: 0.5
          maxSeconds: 30
          tags:
            runtime: tgi
            gpu: rtx4000-ada

        - name: llama-3-1-8b-instruct-vllm-ada
          url: http://api-router-service-staging-api-router-service.staging.svc.cluster.local:8080
          model: llama-3.1-8b-instruct-ada
          rate: 0.5
          maxSeconds: 30
          tags:
            runtime: vllm
            gpu: rtx4000-ada

        # TRT-LLM Models (OpenAI-compatible via trtllm-serve)
        - name: llama-3-1-8b-instruct-trtllm-ada
          url: http://api-router-service-staging-api-router-service.staging.svc.cluster.local:8080
          model: llama-3-1-8b-instruct-trtllm-ada
          rate: 0.5
          maxSeconds: 30
          tags:
            runtime: trtllm
            gpu: rtx4000-ada

        # Multi-Model 3x8B Triton Ensembles (via OpenAI API translation)
        - name: mm-blackwell-1-llama
          url: http://api-router-service-staging-api-router-service.staging.svc.cluster.local:8080
          model: mm-blackwell-1-llama
          rate: 0.5
          maxSeconds: 30
          tags:
            runtime: trtllm-triton
            gpu: rtx6000-blackwell
            ensemble: llama_ensemble

        - name: mm-blackwell-1-mistral
          url: http://api-router-service-staging-api-router-service.staging.svc.cluster.local:8080
          model: mm-blackwell-1-mistral
          rate: 0.5
          maxSeconds: 30
          tags:
            runtime: trtllm-triton
            gpu: rtx6000-blackwell
            ensemble: mistral_ensemble

        - name: mm-blackwell-1-qwen
          url: http://api-router-service-staging-api-router-service.staging.svc.cluster.local:8080
          model: mm-blackwell-1-qwen
          rate: 0.5
          maxSeconds: 30
          tags:
            runtime: trtllm-triton
            gpu: rtx6000-blackwell
            ensemble: qwen_ensemble

resources:
  limits:
    cpu: "2"
    memory: 4Gi
  requests:
    cpu: 500m
    memory: 2Gi

serviceMonitor:
  enabled: true
  interval: 15s
  labels:
    release: kube-prometheus-stack

# Prefer CPU nodes (avoid GPU nodes)
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
            - key: lke.linode.com/pool-id
              operator: NotIn
              values:
                - "785977"  # GPU node pool
                - "785978"  # GPU node pool
